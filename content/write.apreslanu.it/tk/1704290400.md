---
date: '2024-01-03T15:00:00Z'
source: https://write.apreslanu.it/tk/les-vertus-politiques-du-scraping
title: les vertus politiques du scraping
---

À la suite de deux articles de The Markup sur le *scraping*, j’en profite pour revenir sur ce morceau important du web et articuler quelques clés de lecture. Qu’est-ce c’est ? Pourquoi est-ce important ? Et quelques contextes d’usage avec leurs enjeux.

Il y a quelques approximations par esprit de concision. Pardon par avance.

<!--more-->

## Qu’est-ce que c’est ?

Le *scraping* est l’opération consistant à extraire systématiquement de l’information d’un ensemble de pages web (un site en particulier, une pelote de liens, etc.) pour constituer un jeu de données (les données, les changements dans ces données, etc.) qui pourra éventuellement servir à produire une nouvelle information (comparaison, vue macro, etc.). Une fois automatisée, cela permet de changer, l’échelle en quantité et en temporalité, des informations provenant du web. Cela en fait un élément important de l’écologie de la connaissance dans notre univers hypermédiatique.

Le *scraping* mobilise les tâches suivantes :

- naviguer sur des pages web, c.-à-d. au format *HTML*,
- extraire l’information de ces pages,
- et de l’organiser sous la forme d’une base de données, un fichier ou des fichiers, bref, restructurer l’information différement.

Le degré zéro est de le faire soit même à la main. Il y a longtemps, il y avait même des extensions pour navigateurs, par exemple [navicrawler] du Medialab de Sciences Po, pour aider à cela. La plupart du temps, c’est un *script* dans un langage comme *python* qui permet d’automatiser la partie répétitive.

Pour abstraire et généraliser l’extraction d’information, il faut souvent une compétence de compréhension de la structure d’une page web, c.-à-d. savoir lire du HTML, et en comprendre la syntaxe. Bien heureusement, le web est une technologie fondamentalement ouverte et tous les navigateurs permettent d’afficher le code source d’une page en deux clics (clic droit, voir le code source, clic gauche). Si vous entendez parler d’« inspecter le code source », c’est cela, une manipulation qui permet de relier un élément visuel avec un morceau de langage technique décrivant cet élément. Le jeu est alors de trouver le motif permettant de rassembler tous les éléments semblables.

Certains éditeurs de site web cherchent à empêcher ce genre d’opération, il n’est alors pas rare d’utiliser à nouveau un script qui va simuler des séries d’actions humaines dans un navigateur web. Redonnant ainsi un sens au terme de *[user agent]* qui permettait d’identifier un navigateur comme un instrument d’agissement (ou d’agentivité) d’un utilisateur.

[navicrawler]: https://medialab.sciencespo.fr/en/tools/navicrawler/
[user agent]: https://en.wikipedia.org/wiki/User_agent
[Browse.ai]: https://www.browse.ai/

## Pourquoi est-ce important ?

Le *scraping* et sa continuité de pratiques sont importants, car dans le paradigme marketing actuel de l’intelligence artificielle, ce qui est automatisable a une bonne probabilité d’être recyclé comme un service payant avec un travail qui est surtout de la conception d’interface utilisateur, mais comme le design ne paie plus on parle de « robot » et d’« intelligence artificielle ». [Browse.ai] est un exemple de ce genre de maquillage. D’ailleurs, tout cela pourrait être fait avec du travail humain en exploitant un service comme *Mechanical Turk* d’Amazon et cela resterait du *scraping*. La différence notable est la responsabilité de l’usage de technologie et la volonté de fermer les yeux, ou non, sur les conditions d’extraction et d’exploitation du travail. Dans ce contexte, la notion de compétence est également importante, car, bien que technique, savoir lire du HTML est une connaissance nécessaire et normalement relativement facile d’accès. On n’est pas dans des affres de complexité et un minimum de pédagogie fait l’affaire. La promesse ~~du no-code~~ de l’intelligence artificielle est une barrière à cela et empêche la résolution du moindre problème ainsi que l’identification d’erreurs. C’est une dépendance directe au bon vouloir d’une entreprise/d’un service en fonction des orientations managériales et du marché.
Dit autrement, le *scraping* est un bon cas pour ouvrir les différentes problématiques sociales invisibilisées dans le paradigme dans lequel nous sommes plongés depuis quelques années et accélérer par le succès commercial de [OpenAI].

[OpenAI]: https://write.apreslanu.it/tk/tag:openai

L’autre point important est la complexification des technologies web. La professionnalisation des métiers du web et l’économie numérique alimentant un besoin de produire de plus en plus de pages amènent à un amoncellement de nouvelles solutions pour résoudre des problèmes sauf que chaque solution vient avec ses propres problèmes. Pour mettre une page web en ligne, on est bien loin du glisser-déposer d’un fichier vers le FTP fourni gratuitement avec son accès à Internet. Souvent, pas toujours, un site web est plus une surcouche sur une API, un point d’accès programmatique, qui permet de générer des pages et d’avoir une gestion plus dynamique de son contenu que le modèle standard de la page web. Par exemple, plutôt que de faire une page différente par personne côté serveur, on va demander au navigateur d’aller récupérer des informations en parallèle et le laisser modifier la page à un endroit qui sera indiqué par avance. Schématiquement. Une solution de repli pour l’extraction d’information est alors d’aller la chercher dans l’API. On s’éloigne alors petit à petit du *scraping* et de ce que percoit normalement un navigateur et donc du contexte de lecture ainsi que les différentes transformations possibles. L’information récupérée de cette façon est souvent déjà structurée et ainsi plus propre.


## Quels sont les enjeux ?

### commerciaux

Commençons par les choses qui fâchent. Le *scraping* a une fonction importante dans l’économie numérique. Les premiers comparateurs de prix utilisaient du *scraping* pour alimenter leurs contenus avant l’avènement du hangar global. Marc Zuckerberg [a scrapé les trombinoscopes][1], les *facebooks*, de sa fac pour en faire une compétition assez malsaine et à la mode à l’époque. Les startups sont friandes de données personnelles laissées à l’air libre pour alimenter des bases de données prospectives. Laisser son email en clair sur une page web, c’est s’assurer de retrouver sa boite inondée de publicités. Avoir un profil public github, la plateforme de Microsoft pour publier du code, c’est aussi la garantie de se faire prospecter de façon régulière à propos de nouveaux projets crypto.

À ce titre, la [CNIL] est sur le coup et rappelle que c’est interdit. Par contre, aller piller les autres boîtes, c’est une autre histoire. Cousin proche du *scraping*, il y a un *crawling* qui indexe le contenu des pages pour les indexer dans un moteur de recherche. La différence, c’est peut-être que le *crawling* est anticipé, et optimisé, par les éditeurs de site web. Ça s’appelle du SEO, ce n’est pas beau à voir et c’est une tout autre histoire.

[1]: https://www.vox.com/2018/4/10/17220290/mark-zuckerberg-facemash-testimony
[CNIL]: https://www.cnil.fr/fr/la-reutilisation-des-donnees-publiquement-accessibles-en-ligne-des-fins-de-demarchage-commercial

Dans une autre mesure, la possibilité de scraper le web est un dommage collatéral de la volonté des acteurs du web commercial à extraire une valeur économique de l’attention des internautes. Afin de pouvoir assurer l’exposition à des publicités, [Google cherche ainsi à contrôler la lecture d’une page web][web integrity] avec tout un tas de complications dont l’excuse est l’intégrité de ce qui est déclenché. C’est assez fallacieux et c’est un problème qui se mord la queue dans la mesure où le principal risque est les programmes malveillants qui se propagent par l’intermédiaire du réseau des bannières publicitaires. C’est une bataille du web en cours et le soutien, par l’usage, de Firefox est vital.

[web integrity]: https://arstechnica.com/gadgets/2023/07/googles-web-integrity-api-sounds-like-drm-for-the-web/

### recherche

Côté recherche, il y a diverses problématiques allant de la conservation à l’analyse du langage naturel ou bien l’analyse des réseaux sociaux. 

La plus importante est l’archivage du web et sa conservation. De la même façon que les *crawlers* commerciaux, l’enjeu est de conserver de façon intacte le maximum de choses possibles. Cela concerne des projets comme l’emblématique [archive.org] et [l’archivage du web] de la BnF et de l’INA. 

[archive.org]: https://archive.org/
[l’archivage du web]: https://larevuedesmedias.ina.fr/larchivage-du-web-un-outil-pour-comprendre-internet

Une autre problématique est l’analyse des controverses et le champ précédent des *digital methods* qui utilisent les matériaux comme un matériau pour construire des cartographies. C’est beaucoup plus que cela, mais il y a des livres très bien sur le sujet comme [Controversy Mapping] de Venturini et Munk.

[Controversy Mapping]: https://www.goodreads.com/book/show/58153939-controversy-mapping?ac=1&from_search=true&qid=zVxbIVtJrf&rank=3

### journalisme

https://themarkup.org/hello-world/2023/12/16/how-elon-musk-is-trying-to-make-web-scraping-dangerous-again

https://themarkup.org/news/2020/12/03/why-web-scraping-is-vital-to-democracy

The Markup est un média US dont la thématique est la technologie. N’en faisant pas seulement un sujet, les journalistes de cette rédaction mobilisent régulièrement des méthodologies d’extraction d’information qu’il serait laborieux de faire manuellement. À ce titre, ils soulignent que le *scraping* du web est important d’un point de vue démocratique et central dans certaines de leurs enquêtes. Cela dérange assez les gros acteurs pour que cela se termine devant la justice avec de gros enjeux de régulation.

La question que je me pose alors est le lien entre pratique du *scraping* dans les rédactions françaises et la faiblesse du journalisme de données, en tant que champ, en France. Si vous êtes journaliste et que vous scrapez le web pour vos articles, cela m’intéresse d’en discuter dans le cadre d’une étude au long cours sur vos pratiques.

### société civile

La transparence et l’accès de l’information sont également importants pour la société civile et l’existence d’un écosystème citoyen qui ne soient pas dans une confrontation constante avec la sphère marchande et la sphère administrative.

Laisser la possibilité de construire de l’information publique et de nouveaux services, comme vite ma dose, est un signe de santé démocratique, car il est alors possible de construire des communs et, enfin, orienter des technologies vers plus d’inclusion, de solidarité et de compréhension des collectifs.

L’association UFC-Que Choisir construit ainsi une [cartographie des drives][ufc] à partir d’un *scraping* des sites des différentes enseignes. À partir de là, il fournit un indicateur de coût du panier moyen selon les profils de ménage. Avec un peu de travail, il est possible de scraper à nouveau ces pages pour les [transformer en données tabulaires][ufc-csv] permettant des analyses alternatives et de nouvelles mises en récit.

[ufc]: https://www.quechoisir.org/carte-interactive-drives-n21243/
[ufc-csv]: https://framagit.org/taniki/ufc-drives

## conclusion

Le *scraping* est symbolique de la possibilité de prendre du recul, d’avoir une vue d’ensemble, à propos du web et de ce qui y circule. Certainement pas une pratique quotidienne pour l’internaute moyen, c’est un indicateur de la frontière entre ce qui est ouvert ou ne l’est pas sur le web. À ce titre, sa pratique permet de mesurer la surface du web comme *espace public*. Une trajectoire vers moins de contenus qui seraient disponibles au regard des internautes par cet intermédiaire est indicateur d’une opacité de l’information. Les grandes entreprises technologiques, et les gouvernements, en parallèle, bénéficient d’une grande transparence sur nos données individuelles. Comme dirait [Cory Doctorow à propos de l’interopérabilité][internet con], le *scraping* est une condition nécessaire, mais pas suffisante pour un futur désirable où Internet reste omniprésent. Autrement dit, Internet ne peut pas être une technologie sociale si le web ne reste pas ouvert et s’il n’y a pas la possibilité de construire une écologie de la connaissance où le contrôle technique ne serait pas du côté des citoyens.

[internet con]: https://craphound.com/category/internetcon/

Du côté de la production de contenus, de pages et de sites web, il est ainsi important d’avoir cette intention du web comme commun et espace public. La prolifération des applications pour smartphones (à 99,9 % réservées aux jardins/enclos d’Apple et de Google) est ainsi un grand pas vers la clôture du web et par extension de l’information comme moteur de la démocratie.

type : #analyse
sujets : #openweb #scraping

---

Merci d'avoir lu ce texte ! On peut [en discuter sur mastodon](https://social.apreslanu.it/@tk). Pour être informé·e lors de la parution de nouveaux articles, [abonnez-vous au fil rss](https://write.apreslanu.it/tk/feed/).